{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"internship_notebook_guide.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yji7A7eE2ptm","colab_type":"text"},"source":["# WELCOME\n","I'm Tan Chuan Xin. This project is about the SpaceNet6 challenge. "]},{"cell_type":"markdown","metadata":{"id":"eR3mZarIFXkz","colab_type":"text"},"source":["![alt text](https://miro.medium.com/max/19034/1*F7vFbcWflRpptmmnvWMmpg.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"l6qha-F_B0sW","colab_type":"text"},"source":["This notebook shall serve as a guide towards explaining what the keras_12 series notebooks do, and how the code works in general. \n","\n","Do read the guide below in relation to the code that can be found after it. Open up the table of contents on the left hand side to view the notebook structure by headings for easier navigation. It helps a lot.\n"]},{"cell_type":"markdown","metadata":{"id":"3xo8c6ZRBvfv","colab_type":"text"},"source":["## Guide on using the keras_12 notebook template"]},{"cell_type":"markdown","metadata":{"id":"7dLwkKx7avCS","colab_type":"text"},"source":["### **0. GPU use**\n","Before running anything, if the session requires a GPU, it must be selected from the start. Switching halfway will cause all local variables to be lost. \n","\n","If the GPU is not needed, do not connect to it. There are limits placed on GPU usage time, which will be reset every 1-1.5 days. Usage is limited to a few hours each day. Error message when connecting will look like this: "]},{"cell_type":"markdown","metadata":{"id":"SFFEMZUKbw2k","colab_type":"text"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"TulWffSIb0Ii","colab_type":"text"},"source":["The more you use the slower the GPU assigned to you, since this is a free platform \n","\n","To connect to a GPU, \n","* Click 'Runtime' at the top left menu bar\n","* Click 'Change Runtime Type'\n","* Dropdown menu of 'Hardware Accelerator', click 'GPU'\n","* Click 'Save'\n","* Top right menu bar, click 'Connect' or 'Reconnect' accordingly "]},{"cell_type":"markdown","metadata":{"id":"myAAp6D28pmD","colab_type":"text"},"source":["### **1. Change the global stuff**\n","The global stuff are things that will be set and used for the entire notebook. Do check these parameters because they affect a lot of subsequent functions and should only have their value initialized once\n","  * **DATA \\ What is this notebook about**\n","    * Change the description to fit the goal of this particular notebook\n","  * **DATA \\ Global \\ Constants, IMG**\n","    * seed = 42\n","      * Used to seed a predictable set of random values for random operations. Can change this if you want, but it's the meaning of life.\n","    * IMG_HEIGHT = 256, IMG_WDITH = 256\n","      * Check prior to executing the notebook. It will affect the directories and images that are going to be used subsequently for the entire notebook. 256x256 is used because the original 900x900 image is rather big and this seems a suitable size for fast enough operations, but you can change this any way you like. \n","    * PATH_SPACENET6 = Path(root_dir + '/spacenet6')\n","      * The directory that all files related to this project is in. Change if need\n","    * PATH_METRICS = PATH_SPACENET6/'metrics_keras_12.csv'\n","      * The path to the metrics csv file that will keep track of vital information of each model. Change when needed \n","\n","  * **DATA \\ Global \\ SIZE, IDX**\n","    * SIZE_ALL = 3401\n","      * Do not change unless really necessary, this is the number of image files in the train_all folder for the training images\n","    * SIZE_TRAIN = XXX, SIZE_TEST = XXX\n","      * Note that SIZE_TRAIN + SIZE_TEST = 3401 should be obeyed. We are using some of the training images as test images because the actual test images supplied do not come with a ground truth, hence we cannot score our model\n","    * TRAIN_IDX, TEST_IDX\n","      * These are indices on an array of 3401 that are randomly generated based on the given SIZE_TRAIN and SIZE_TEST. However these indices will be saved into a temporary .npy file in the root path of PATH_SPACENET6 to allow for fast and standardized trial and error using the same images, to minimise the number of variables being changed at any one time. \n","      * Three sets have been supplied already in train-test pairs, namely 800-80, 1600-160, 3200-201. The last one is the full set\n","\n","  * **DATA \\ Global \\ Rotation file**\n","    * This is used to identify the rotation flag for the data. Don't need to change"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kMhY9uh8Mue6"},"source":["### **2. Helper Functions**\n","These are functions that perform some small, non-critical tasks. Usually for I/O and visualization operations\n","  * **DATA \\ Helper functions**\n","    * rotation()\n","      * This function is used to rotate a SAR Image specifically. Because the SAR satellite has flightpaths from North-South and South-North, the angle of image capture is different. This affects the image greatly, and rotation might be necessary to standardize image directions. The rotation is determined by the rotation file and the image file name\n","      * function signature\n","       * ```in: filename: string, img: np.ndarray```\n","       * ```out: img: np.ndarray```\n","    * plot_XY()\n","      * This function is used to plot an image (X) together with its mask (Y), as verification that the mask has been created succesfully. Note that the image to be passed in should be in standard image format (height x width x channel). \n","      * Note also that X and Y are arrays of images. Passing in the optional parameter _index_ will choose that specific index for plotting\n","      * function signature\n","        * in: plotname: string, X: np.ndarray, Y: np.ndarray, X_name: string, Y_name: string, index=None: int\n","        * out: None\n","    * plot_XY_preds()\n","      * This function is used to plot an image (X) together with its mask (Y) and the prediction for the image (P), as a comparison for the prediction vs the ground truth. Note that the image to be passed in should be in standard image format (height x width x channel)\n","      * Note also that X and Y and P are arrays of images. Passing in the optional parameter _index_ will choose that specific index for plotting      \n","      * function signature\n","        * ```in: plotname: string, X: np.ndarray, Y: np.ndarray, P: np.ndarray, X_name: string, Y_name: string, P_name: string, index=None: int```\n","        * ```out: None```\n","    * plot_history()\n","      * This function is used to plot the metrics of the model. It will plot out loss, iou_score and f1_score for the training, validation and test set\n","      * function signature\n","        * ```in: history: history object from model```\n","        * ```out: None```\n","    * store_metrics()\n","      * This function is used to store various information regarding a trained model into a csv file. The csv file makes it easier to compare information, rather than browsing across different notebooks, or even within one long notebook\n","      * The bit of code above the function will create the desired metric file if not created\n","      * function signature\n","        * ```in: test_loss: float32, test_iou_score: float32, test_f1_score: float32, val_loss: float32, val_iou_score: float32, val_f1_score: float32, train_loss: float32, train_iou_score: float32, train_f1_score: float32,IMG_TYPE: string, IMG_ROTATED: boolean, MODEL_NAME: string, MODEL_TYPE: string, MODEL_PRETRAIN: string, MODEL_LOSS_FUNCTION: string, MODEL_COMMENTS: string, MODEL_BATCH_SIZE: int64, MODEL_EPOCHS: int64, SIZE_TRAIN: int64, SIZE_TEST: int64, PATH_METRICS: PosixPath```\n","        * ```out: None```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xNygYJJKVEvn"},"source":["### **3. Generator**\n","These are functions that are integral for creating the data to be used for the models, creating the models themselves. \n","\n","Generators return every single variable that might be necessary, so that these variables can be overwritten as a quick and dirty way of testing out various things. Therefore the input and return variables can be fairly long \n","\n","The generators are also pipelined such that each function is called successively. \n","\n","  * **DATA \\ Generator**\n","    * gen_input_image_details()\n","      * This function is used to set the image specifications for the model that we are training. It makes use of the global lookup dictionary IMG_TYPE_LOOKUP to determine some of the image specifications\n","        * function signature\n","          * ```in: IMG_TYPE: string, IMG_ROTATED: boolean, IMG_HEIGHT=IMG_HEIGHT: int64, IMG_WIDTH=IMG_WIDTH: int64, IMG_TYPE_LOOKUP=IMG_TYPE_LOOKUP: dictionary```\n","          * ```out: IMG_TYPE: string, IMG_HEIGHT: int64, IMG_WIDTH: int64, IMG_ROTATED: boolean, IMG_CHANNELS: int64, IMG_DTYPE_SRC: type, IMG_DTYPE: type```\n","    * gen_paths()\n","      * This function is used to generate the various paths to traverse the directory for files that are needed. If the directory does not exist, it will create these directories. \n","      * function signature\n","        * ```in: IMG_TYPE: string, IMG_ROTATED: boolean, IMG_HEIGHT=IMG_HEIGHT: int64, IMG_WIDTH=IMG_WIDTH: int64```\n","        * ```out: PATH_SRC: PosixPath, PATH_SRC_IMAGE: PosixPath, PATH_SRC_MASK: PosixPath, PATH_DATA: PosixPath, PATH_IMAGE: PosixPath, PATH_MASK: PosixPath, PATH_SAMPLE: PosixPath, PATH_SAVED_MODELS: PosixPath```\n","    * gen_resized_images()\n","      * This function is used to generate the required images based on the IMG_TYPE, IMG_HEIGHT, IMG_WIDTH and IMG_ROTATED variables. If the required images are already prepared, it will not generate them again. If they are not ready, directories will be created and images will be saved\n","      * function signature\n","        * ```in: PATH_SRC_IMAGE: PosixPath, PATH_SRC_MASK: PosixPath, PATH_IMAGE: PosixPath, PATH_MASK: PosixPath, IMG_HEIGHT=IMG_HEIGHT: int64, IMG_WIDTH=IMG_WIDTH: int64```\n","        * out: ```None```\n","    * gen_train_test_set()\n","      * This function will create the train-test set based on images specifications we want, and the global SIZE variables. Remember that the 'test' images actually come from the train images because we need the ground truth\n","      * To speed up testing, it will load in sample data if they are available. Sample data is made available after the first run of code, where the images will be generated based on random index, and saved in a .npy file format. This greatly speeds up data retrieval and standardises the random data used across experiments \n","      * If sample data is not available, based on the SIZE specified, random IDX files will be generated and random SAMPLE data will be created based on the IDX\n","      * function signature\n","        * ```in: PATH_IMAGE: PosixPath, PATH_MASK: PosixPath, IMG_CHANNELS: int64, IMG_DTYPE_SRC: type, TRAIN_IDX: int64[], TEST_IDX: int64[]: , SIZE_TRAIN=SIZE_TRAIN: int64, SIZE_TEST=SIZE_TEST: int64, IMG_HEIGHT=IMG_HEIGHT: int64, IMG_WIDTH=IMG_WIDTH: int64```\n","        * ```out: X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray```\n","    * gen_model()\n","      * This function will generate a keras model that is based off the segmentation_models module found from [their github link](https://github.com/qubvel/segmentation_models). Of course, this code can be modified to generate all sorts of models, but in the interest of time only this particular module will be explored\n","      * The model will be generated based on a whole slew of inputs that we are giving the model. Do take note that some aspects of the model will be hardcoded into the gen_model() function. These need to be changed within the function, or modify the function signature to accept these hardcoded parts as variables to be passed on\n","      * function signature\n","        * ```in: MODEL_LOSS: function, MODEL_BATCH_SIZE: int64, MODEL_EPOCHS: int64, MODEL_TYPE: string, MODEL_PRETRAIN: string, MODEL_LOSS_FUNCTION: string, IMG_CHANNELS: int64, MODEL_ID=None: int64, MODEL_COMMENTS='': string```\n","        * ```out: MODEL_OPTIMIZER: string, MODEL_LOSS: function, MODEL_METRICS: function[], MODEL_VAL_SPLIT: float32, MODEL_VAL_OFFSET: int64, MODEL_BATCH_SIZE: int64, MODEL_EPOCHS: int64, MODEL_TYPE: string, MODEL_PRETRAIN: string, MODEL_LOSS_FUNCTION: string, MODEL_COMMENTS: string, MODEL_ID: int64, MODEL_NAME: string, model: keras model```\n","    * gen_model_results()\n","      * This function will essentially be used to train the model that was generated previously, after being given a few more variables to work with. Do note that the callbacks have been hardcoded, and to change it, either change the hardcoded values, or recode the function to accept variables\n","      * This function operates on two modes: training a completely new model, or further train a model based on a pretrained one. \n","        * To further train a saved model, pass in the filename for the model that we want to load. Then provide the number of additional epochs to train the model for. If set to 0, then the model will be loaded without any additional training. \n","      * The model can be saved by setting save_model=True\n","      * function signature\n","        * ```in: PATH_SAVED_MODELS: PosixPath, MODEL_NAME: string, X_train: np.ndarray, Y_train: np.ndarray, MODEL_BATCH_SIZE: int64, MODEL_EPOCHS: int64, MODEL_VAL_SPLIT: float32, load_weights_filename='': string, load_weights_epochs=0: int64, save_model=False: boolean```\n","        * ```out: results: keras results , model: keras model```\n","    * gen_predictions()\n","      * This function generates the predictions for a train-val-test set based on the model that was loaded/trained before and displays the output. If the results of a trained model are available, it plots the various model metrics over the epochs training cycle to visualize the changes in the model and perhaps tweak overfitting etc\n","      * Note that keras takes validation split from the rear index of an input training set, therefore the hardcoded array splicing to seperate train into train-val\n","      * The prediction metrics will then be saved into the target results csv file\n","      * The return will be the predictions as np.ndarray (formatted as image)\n","      * function signature\n","        * ```in: model: keras model, X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, MODEL_VAL_OFFSET: int64, IMG_TYPE: string, IMG_ROTATED: boolean, MODEL_NAME: string, MODEL_TYPE: string, MODEL_PRETRAIN: string, MODEL_LOSS_FUNCTION: string, MODEL_COMMENTS: string, MODEL_BATCH_SIZE: int64, MODEL_EPOCHS: int64, SIZE_TRAIN=SIZE_TRAIN: int64, SIZE_TEST=SIZE_TEST: int64, PATH_METRICS=PATH_METRICS: PosixPath()```\n","        * ```out: preds_train_t: np.ndarray, preds_val_t: np.ndarray, preds_test_t: np.ndarray```"]},{"cell_type":"markdown","metadata":{"id":"7a4RNeLLrOdM","colab_type":"text"},"source":["### **4. Bring in the SAR data**\n","* **DATA \\ Bring in the SAR data, both rotated and un-rotated**\n","\n","Essentially this just works to load in the SAR data that we want to work with and make it available for the rest of the notebook. Just load once and forget about it after. It is more efficient.\n","\n","We load in both datasets in case we need them. This section will call generator functions of \n","* gen_input_image_details()\n","* gen_paths()\n","* gen_resized)images()\n","* gen_train_test_set()\n","\n","Make sure to load in all the data that you might want to train over here! And this data is dependent on the global SIZE_TRAIN and SIZE_ALL variables. Of course, they can be overwritten at this point, but try and keep it in global for easier tracking"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s6JOAVZkf_s_"},"source":["### **5. Experiment with models**\n","* **EXPERIMENT \\ Experiment details**\n","\n","List out what the models here are testing, and some of the key model parameters that have been changed in this notebook relative to the other notebooks, or the previous one.\n","\n","This will serve to keep track of changing model inputs and can be quickly referred to when referencing multiple notebooks.\n","\n","* **EXPERIMENT \\ Experiment XXX**\n","  * This will be some group of experiment based on some key feature. Repeat a few times just to make sure that whatever result comes out is not a fluke. \n","  * It might be good to repeat the same run two times or more\n","\n","\n","* **EXPERIMENT \\ Experiment XXX \\ ID XXX**\n","\n","    * Provide a numerical ID to the model that we are training, so that it is easier to identify later when trying to retrieve a model.\n","\n","    * Standard format to run an experiment has been pushed into the generator functions for ease of use. For something more customizable, you can copy the internal code of each generator function sequentially and modify accordingly. The steps are as follows:\n","      1. gen_input_image_details() - to fix the image specifications for the model\n","        * For SAR, can choose whether to use the rotated images or not using a boolean flag\n","      2. gen_paths() - to fix the paths that will be used for this model\n","      3. Select the data to use from before, when the data was brought in\n","      4. gen_model() - to create the desired model based on the parameters provided to it. Pay special attention to this step, a lot of model parameters are set here\n","      5. gen_model_results() - to train the model. This step can also be used to load in an existing model and train it further or simply use it to predict \n","      6. gen_predictions() - after model has been trained, use it to generate predictions and obtain metrics for the models' performance. Metrics will be saved in the designated csv file\n","      7. plot_XY_preds() - used to plot some of the predictions beside the ground truth as a visual method of seeing the results "]},{"cell_type":"markdown","metadata":{"id":"H7Pk2gbklOeY","colab_type":"text"},"source":["# --------- TEMPLATE BELOW ---------"]},{"cell_type":"markdown","metadata":{"id":"LWBfzqYRoCP5","colab_type":"text"},"source":["# DATA"]},{"cell_type":"markdown","metadata":{"id":"jHSBYcLFYfua","colab_type":"text"},"source":["## What is this notebook about"]},{"cell_type":"markdown","metadata":{"id":"yy3ch0EqY-vJ","colab_type":"text"},"source":["This notebook builds upon the notebook ``` spacenet6_masking.ipynb ```\n","\n","``` spacenet6_masking.ipynb ``` is used to generate all the masks for all 3401 training images, using only the PAN images and geojson_buildings files. These masks will be the same regardless of the original data format, hence it is a run-once-and-forget. \n"]},{"cell_type":"markdown","metadata":{"id":"E-QTFxC-o36k","colab_type":"text"},"source":["**Show work**\n","\n","This notebook shall be used to show some of the work that has already been done in the other notebooks, more specifically the keras_12 series of notebooks\n","\n","Do refer to this to learn how to perhaps use the functions, and also to view the results\n"]},{"cell_type":"markdown","metadata":{"id":"WM0KmAs2JS-A","colab_type":"text"},"source":["## Setup COLAB and imports"]},{"cell_type":"markdown","metadata":{"id":"kHX_9KfVm8MJ","colab_type":"text"},"source":["Mount drive to gain access to files"]},{"cell_type":"code","metadata":{"id":"5sSATmzAJD5F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1596537799959,"user_tz":-480,"elapsed":21260,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"3ded1901-5bcc-4bd0-c3e4-4ba12d7a77c5"},"source":["''' Used to reference the root directory, for directory traversal ''' \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","mount_dir = '/content/gdrive'\n","root_dir = '/content/gdrive/My Drive/Colab Notebooks'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"szrEiH0Cm_iL","colab_type":"text"},"source":["Installations required"]},{"cell_type":"code","metadata":{"id":"-wsLN5sICxFL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":972},"executionInfo":{"status":"ok","timestamp":1596537820464,"user_tz":-480,"elapsed":41098,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"f5efa265-102f-491f-9b8b-76d59b39e2cb"},"source":["! pip install fiona\n","! pip install rasterio\n","! pip install segmentation-models"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting fiona\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/20/4e63bc5c6e62df889297b382c3ccd4a7a488b00946aaaf81a118158c6f09/Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl (14.7MB)\n","\u001b[K     |████████████████████████████████| 14.7MB 310kB/s \n","\u001b[?25hCollecting click-plugins>=1.0\n","  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n","Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona) (7.1.2)\n","Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona) (19.3.0)\n","Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona) (1.15.0)\n","Collecting munch\n","  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n","Collecting cligj>=0.5\n","  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n","Installing collected packages: click-plugins, munch, cligj, fiona\n","Successfully installed click-plugins-1.1.1 cligj-0.5.0 fiona-1.8.13.post1 munch-2.5.0\n","Collecting rasterio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/7e/eed7dfd109fc89ed3cf8b5ed3f26f841b03b92f6ca1c31c4745f938a081b/rasterio-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (18.2MB)\n","\u001b[K     |████████████████████████████████| 18.2MB 237kB/s \n","\u001b[?25hRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from rasterio) (0.5.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (19.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.5)\n","Collecting snuggs>=1.4.1\n","  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.1.1)\n","Collecting affine\n","  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n","Installing collected packages: snuggs, affine, rasterio\n","Successfully installed affine-2.3.0 rasterio-1.1.5 snuggs-1.4.7\n","Collecting segmentation-models\n","  Downloading https://files.pythonhosted.org/packages/da/b9/4a183518c21689a56b834eaaa45cad242d9ec09a4360b5b10139f23c63f4/segmentation_models-1.0.1-py3-none-any.whl\n","Collecting image-classifiers==1.0.0\n","  Downloading https://files.pythonhosted.org/packages/81/98/6f84720e299a4942ab80df5f76ab97b7828b24d1de5e9b2cbbe6073228b7/image_classifiers-1.0.0-py3-none-any.whl\n","Collecting efficientnet==1.0.0\n","  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n","Collecting keras-applications<=1.0.8,>=1.0.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n","Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n","Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ZeYQQoqTwM9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596537905658,"user_tz":-480,"elapsed":80538,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"394fe018-e3e7-44d9-9525-8cff69c13905"},"source":["''' colab recently upgraded their tensorflow packages and some stuff is breaking as of 4 Aug 2020'''\n","''' use this version of tensorflow for now'''\n","! pip install q tensorflow==2.1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting q\n","  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n","Collecting tensorflow==2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n","\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n","\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.9MB 34.7MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.2.0)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.4.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.34.2)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.12.4)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.18.5)\n","Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 42.1MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.15.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.30.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.12.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.9.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.3.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.0.8)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.2.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (49.2.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.17.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1) (2.10.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.7.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=26b729ac41be645d483af818d16aaebe501ec4a29ff7dd8b627649d1286fb85b\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: q, tensorboard, gast, tensorflow-estimator, tensorflow\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed gast-0.2.2 q-2.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cPd4ZPVMnDYH","colab_type":"text"},"source":["Imports required"]},{"cell_type":"code","metadata":{"id":"VXtS25YR61kG","colab_type":"code","colab":{}},"source":["''' dealing with .tif files '''\n","import fiona\n","import rasterio\n","from rasterio.mask import mask\n","from rasterio.plot import show, reshape_as_raster, reshape_as_image\n","\n","''' plotting images '''\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","''' traversing directories '''\n","import os\n","from pathlib import Path\n","\n","''' core '''\n","import numpy as np\n","import pandas as pd\n","\n","''' utilities '''\n","from tqdm import tqdm\n","import copy\n","import datetime\n","\n","''' image processing '''\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import image\n","\n","''' unet model '''\n","import segmentation_models as sm\n","from segmentation_models import Unet\n","from segmentation_models.losses import JaccardLoss, BinaryFocalLoss, DiceLoss, BinaryCELoss\n","from segmentation_models.metrics import IOUScore, FScore"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpv9anqYiiWx","colab_type":"text"},"source":["## Global"]},{"cell_type":"markdown","metadata":{"id":"VeybShj4mP3H","colab_type":"text"},"source":["### Constants, IMG\n","For reference the various image types are as such (amax is a rough gauge) based on the train_all data\n","\n","```\n","type             shape                dtype       gauge of max value    gauge of min value\n","PAN              (1, 900, 900)        uint16      1844                  0\n","PS-RGB           (3, 900, 900)        uint8       255                   0\n","PS-RGBNIR        (4, 900, 900)        uint16      2047                  0\n","RGBNIR           (4, 450, 450)        uint16      2047                  0\n","SAR-Intensity    (4, 900, 900)        float32     92.87763              0.0\n","```\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mj3TjKeYFIu_","colab":{}},"source":["''' get the same sequence for random set every time '''\n","seed = 42\n","np.random.seed = seed\n","tf.seed = seed       \n","\n","''' lookup - define image channels for each image type '''\n","IMG_TYPE_LOOKUP = {\n","    'PAN': {'channels': 1, 'dtype': np.uint16}, \n","    'PS-RGB': {'channels': 3, 'dtype': np.uint8}, \n","    'PS-RGBNIR': {'channels': 4, 'dtype': np.uint16}, \n","    'RGBNIR': {'channels': 4, 'dtype': np.uint16}, \n","    'SAR-Intensity': {'channels': 4, 'dtype': np.float32}, \n","}\n","\n","''' image height and width we are working with for the whole notebook '''\n","IMG_HEIGHT = 256    \n","IMG_WIDTH = 256\n","\n","''' root of all paths '''\n","PATH_SPACENET6 = Path(root_dir + '/spacenet6')\n","PATH_METRICS = PATH_SPACENET6/'metrics_keras_12.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HgdrwGVE9t9Q","colab_type":"text"},"source":["### SIZE, IDX\n","We will be setting the amount of data that we want to use for train/test\n","* There are 3401 images for each image type\n","* The actual spacenet6 \"test folder\" has only uneseen SAR-Intensity data\n","\n","Therefore for training purposes, we will pull out our own \"test data\" from the training set for ease of verifying output \n","\n","Random indexes will be used for training and test"]},{"cell_type":"code","metadata":{"id":"_bBIqZsTpZuQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596036648023,"user_tz":-480,"elapsed":1560,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"fba5a49b-d14b-47e2-fa51-94ca656422cb"},"source":["''' set the various sizes we want to use '''\n","SIZE_ALL = 3401 \n","SIZE_TRAIN = 800           # CHANGE HERE\n","SIZE_TEST = 80            # CHANGE HERE\n","# 800-80, 1600-160, 3200-201\n","\n","''' set the various idexes we want to use '''\n","try:\n","  TRAIN_IDX = np.load(PATH_SPACENET6/('TRAIN_IDX_'+str(SIZE_TRAIN)+'.npy'))\n","  TEST_IDX = np.load(PATH_SPACENET6/('TEST_IDX_'+str(SIZE_TEST)+'.npy'))\n","  print('TRAIN/TEST indexes available. Loading ')\n","except:\n","  ''' Generate the indexes that we will be using for train and test '''  \n","  ALL_IDX = np.arange(SIZE_ALL)\n","\n","  TRAIN_IDX = np.random.choice(ALL_IDX, SIZE_TRAIN, replace=False)\n","  TRAIN_IDX.sort()\n","\n","  ALL_IDX_2 = np.setdiff1d(ALL_IDX, TRAIN_IDX)\n","\n","  TEST_IDX = np.random.choice(ALL_IDX_2, SIZE_TEST, replace=False)\n","  TEST_IDX.sort()\n","\n","  np.save(PATH_SPACENET6/('TRAIN_IDX_'+str(SIZE_TRAIN)), TRAIN_IDX)\n","  np.save(PATH_SPACENET6/('TEST_IDX_'+str(SIZE_TEST)), TEST_IDX)\n","  print('TRAIN/TEST indexes not available. Generated and saved ')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRAIN/TEST indexes available. Loading \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X0JJ5nNB9yDG","colab_type":"text"},"source":["### Rotation file\n","These are the images that have been rotated"]},{"cell_type":"code","metadata":{"id":"NvIhxpOrv-u_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596036651594,"user_tz":-480,"elapsed":1504,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"33be6a55-2848-4784-beae-8fe2b1069830"},"source":["TO_ROTATE = []\n","PATH_ORIENTATIONS = PATH_SPACENET6/'spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam/SummaryData/SAR_orientations.txt'\n","\n","orientation_file = open(PATH_ORIENTATIONS, \"r\")\n","for line in orientation_file:\n","  tilename = line.split(' ')[0]\n","  orientation_flag = int(line.split(' ')[1])\n","\n","  if orientation_flag == 1:\n","    TO_ROTATE.append(tilename)\n","\n","TO_ROTATE.sort()\n","\n","print('orientation file loaded')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["orientation file loaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gnf7t55AZ1Hx","colab_type":"text"},"source":["## Helper functions"]},{"cell_type":"markdown","metadata":{"id":"IAUhkoJn-EjP","colab_type":"text"},"source":["### rotation()"]},{"cell_type":"code","metadata":{"id":"cG_54chA-HnS","colab_type":"code","colab":{}},"source":["def rotation(filename, img):\n","  try:\n","    if filename.split(IMG_TYPE+'_')[1].split('_tile_')[0] in TO_ROTATE:\n","      return np.fliplr(np.flipud(img))\n","    else:\n","      return img\n","  except: \n","    pass\n","\n","  try:\n","    if filename.split('Buildings_')[1].split('_tile_')[0] in TO_ROTATE:\n","      return np.fliplr(np.flipud(img))\n","    else:\n","      return img\n","  except: \n","    pass\n","      \n","  \n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4H4ZxYl2aVqy","colab_type":"text"},"source":["### plot_XY()"]},{"cell_type":"code","metadata":{"id":"ox3X9qRMbXWf","colab_type":"code","colab":{}},"source":["def plot_XY(plotname, X, Y, X_name, Y_name):\n","  ix = np.random.randint(0, len(X))\n","  print(\"index: {}\".format(ix))\n","  print(plotname, \" set\")\n","\n","  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,5))\n","  show(reshape_as_raster(X[ix]), ax=ax1, title=X_name)\n","  show(reshape_as_raster(Y[ix]), ax=ax2, title=Y_name)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKwr0YlaYqCv","colab_type":"text"},"source":["### plot_XY_preds()"]},{"cell_type":"code","metadata":{"id":"McyTdW5mgaw6","colab_type":"code","colab":{}},"source":["def plot_XY_preds(plotname, X, Y, P, X_name, Y_name, P_name, index=None):\n","  if index==None:\n","    ix = np.random.randint(0, len(X))\n","  else:\n","    ix = index\n","  print(\"index: {}\".format(ix))\n","  print(plotname, \" set\")\n","\n","  fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,5))\n","\n","  if X.dtype == 'float32':\n","    show(reshape_as_raster(  (X[ix] - np.amin(X)) / (np.amax(X) - np.amin(X))   ), ax=ax1, title=X_name)\n","  else:\n","    show(reshape_as_raster(X[ix]), ax=ax1, title=X_name)\n","  show(reshape_as_raster(Y[ix]), ax=ax2, title=Y_name)\n","  show(reshape_as_raster(P[ix]), ax=ax3, title=P_name)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wg_oRlVtYtj1","colab_type":"text"},"source":["### plot_history()"]},{"cell_type":"code","metadata":{"id":"LtUtna2aRK-r","colab_type":"code","colab":{}},"source":["def plot_history(history):\n","  fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,5))\n","\n","  ax1.plot(history['loss'], label='train')\n","  ax1.plot(history['val_loss'], label='val')\n","  ax1.title.set_text('loss')\n","  ax1.legend()\n","\n","  ax2.plot(history['iou_score'], label='train')\n","  ax2.plot(history['val_iou_score'], label='val')\n","  ax2.title.set_text('iou_score')\n","  ax2.legend()\n","\n","  ax3.plot(history['f1-score'], label='train')\n","  ax3.plot(history['val_f1-score'], label='val')\n","  ax3.title.set_text('f1-score')\n","  ax3.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAUDS_MUY9FZ","colab_type":"text"},"source":["### store_metrics()\n","Save into the metrics file"]},{"cell_type":"code","metadata":{"id":"qEHfG993EiSi","colab_type":"code","colab":{}},"source":["try: \n","  METRICS_HISTORY = pd.read_csv(PATH_METRICS)\n","except:\n","  TIMESTAMP = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n","  IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS = 'trial', 'trial', 'trial', 'trial', 'trial', 'trial', 'trial'\n","  MODEL_BATCH_SIZE, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST, train_loss, train_iou_score, train_f1_score, val_loss, val_iou_score, val_f1_score, test_loss, test_iou_score, test_f1_score = 0,0,0,0,0,0,0,0,0,0,0,0,0\n","\n","  METRICS_HISTORY = pd.DataFrame(np.array([[TIMESTAMP, IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_BATCH_SIZE, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST, \n","                              test_loss, test_iou_score, test_f1_score,\n","                              val_loss, val_iou_score, val_f1_score, \n","                              train_loss, train_iou_score, train_f1_score\n","                              ]]),\n","                    columns=['timestamp', 'img_type', 'img_rotated', 'model_name', 'model_type', 'pretrain', 'loss_function', 'comments', 'batch_size', 'epochs', 'size_train', 'size_test', \n","                              'test_loss', 'test_iou_score', 'test_f1-score',\n","                              'val_loss', 'val_iou_score', 'val_f1-score', \n","                              'train_loss', 'train_iou_score', 'train_f1-score'\n","                              ],\n","                    )\n","  METRICS_HISTORY.to_csv(PATH_METRICS, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEWmvB_wEmAT","colab_type":"code","colab":{}},"source":["def store_metrics(test_loss, test_iou_score, test_f1_score, val_loss, val_iou_score, val_f1_score, train_loss, train_iou_score, train_f1_score, IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_BATCH_SIZE, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST, PATH_METRICS):\n","  TIMESTAMP = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n","\n","  ''' create another df that looks just like that and concat with ''' \n","  new_metrics = pd.DataFrame(np.array([[TIMESTAMP, IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_BATCH_SIZE, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST,\n","                             test_loss, test_iou_score, test_f1_score,\n","                             val_loss, val_iou_score, val_f1_score, \n","                             train_loss, train_iou_score, train_f1_score                             \n","                             ]]),\n","                   columns=['timestamp', 'img_type', 'img_rotated', 'model_name', 'model_type', 'pretrain', 'loss_function', 'comments', 'batch_size', 'epochs', 'size_train', 'size_test', \n","                            'test_loss', 'test_iou_score', 'test_f1-score',\n","                            'val_loss', 'val_iou_score', 'val_f1-score',\n","                            'train_loss', 'train_iou_score', 'train_f1-score'                            \n","                            ],\n","                  )\n","\n","  new_metrics.to_csv(PATH_METRICS, mode='a', header=False, index=False)\n","  print(\"saved model metrics\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRDF84php1T3","colab_type":"text"},"source":["## Generator"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YYLNlDaKV6B_"},"source":["### gen_input_image_details()\n","Set the specifications for the image input into the model"]},{"cell_type":"code","metadata":{"id":"pASaE_o5mWfS","colab_type":"code","colab":{}},"source":["def gen_input_image_details(IMG_TYPE, IMG_ROTATED, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_TYPE_LOOKUP=IMG_TYPE_LOOKUP):\n","  print('gen_input_image_details()')\n","  \n","  IMG_CHANNELS = IMG_TYPE_LOOKUP[IMG_TYPE]['channels']\n","  IMG_DTYPE_SRC = IMG_TYPE_LOOKUP[IMG_TYPE]['dtype']\n","  IMG_DTYPE = np.uint8               \n","\n","  print('IMG_TYPE: {}, IMG_HEIGHT: {}, IMG_WIDTH: {}, IMG_ROTATED: {}, IMG_CHANNELS: {}, IMG_DTYPE_SRC: {}, IMG_DTYPE: {}\\n'.format(IMG_TYPE, IMG_HEIGHT, IMG_WIDTH, IMG_ROTATED, IMG_CHANNELS, IMG_DTYPE_SRC, IMG_DTYPE))\n","\n","  return (IMG_TYPE, IMG_HEIGHT, IMG_WIDTH, IMG_ROTATED, IMG_CHANNELS, IMG_DTYPE_SRC, IMG_DTYPE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MwOsnUrZOE8","colab_type":"text"},"source":["### gen_paths()\n","Setting the paths that we require, which will change based on the image type and image size specified"]},{"cell_type":"code","metadata":{"id":"1L-M4qIfjQvf","colab_type":"code","colab":{}},"source":["def gen_paths(IMG_TYPE, IMG_ROTATED, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH):\n","  print('gen_paths()')\n","\n","  ''' set the original data source folder '''\n","  PATH_SRC = PATH_SPACENET6/'spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam'\n","  PATH_SRC_IMAGE = PATH_SRC/IMG_TYPE\n","  PATH_SRC_MASK = PATH_SRC/'_mask'\n","\n","\n","  ''' change in accordance to the image type and dimensions '''\n","  COMBI = IMG_TYPE + '_' + str(IMG_HEIGHT) + 'x' + str(IMG_WIDTH)\n","\n","\n","  ''' set the directories for this combination '''\n","  if IMG_ROTATED:\n","    PATH_DATA = PATH_SPACENET6/('spacenet6_data_rotated/' + COMBI)\n","    PATH_DATA.mkdir(parents=True, exist_ok=True)\n","  else:\n","    PATH_DATA = PATH_SPACENET6/('spacenet6_data/' + COMBI)\n","    PATH_DATA.mkdir(parents=True, exist_ok=True)\n","\n","  PATH_IMAGE = PATH_DATA/'image'\n","  PATH_IMAGE.mkdir(parents=True, exist_ok=True)\n","\n","  PATH_MASK = PATH_DATA/'mask'\n","  PATH_MASK.mkdir(parents=True, exist_ok=True)\n","\n","  PATH_SAMPLE = PATH_DATA/'sample'\n","  PATH_SAMPLE.mkdir(parents=True, exist_ok=True)\n","\n","  PATH_SAVED_MODELS = PATH_DATA/'saved_models'\n","  PATH_SAVED_MODELS.mkdir(parents=True, exist_ok=True)\n","\n","  print('PATH_SRC: {}, \\nPATH_SRC_IMAGE: {}, \\nPATH_SRC_MASK: {}, \\nPATH_DATA: {}, \\nPATH_IMAGE: {}, \\nPATH_MASK: {}, \\nPATH_SAMPLE: {}, \\nPATH_SAVED_MODELS: {}\\n'.format(PATH_SRC, PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_DATA, PATH_IMAGE, PATH_MASK, PATH_SAMPLE, PATH_SAVED_MODELS))\n","\n","  return (PATH_SRC, PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_DATA, PATH_IMAGE, PATH_MASK, PATH_SAMPLE, PATH_SAVED_MODELS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQP-9v1bBDXm","colab_type":"text"},"source":["### gen_resized_images()\n","This will check if the required resized images have already been created. If not, it will create it."]},{"cell_type":"code","metadata":{"id":"3TxOPubixufU","colab_type":"code","colab":{}},"source":["def gen_resized_images(PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_IMAGE, PATH_MASK, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH):\n","  print('gen_resized_images()')\n","  \n","  \n","  def resize_and_save(img_src_path, img_fname, img_save_path):\n","    img = rasterio.open(img_src_path/img_fname)\n","\n","    ''' use the metadata from the original image '''\n","    transform = img.transform\n","    meta = img.meta.copy()\n","\n","    ''' resize the image '''\n","    resized_img = img.read()\n","    # print(resized_img.shape, resized_img.dtype, 'amax: {}'.format(np.amax(resized_img)), 'amin: {}'.format(np.amin(resized_img)))\n","    resized_img = reshape_as_image(resized_img)\n","    # print(resized_img.shape, resized_img.dtype, 'amax: {}'.format(np.amax(resized_img)), 'amin: {}'.format(np.amin(resized_img)))\n","\n","    resized_img = tf.image.resize(resized_img, [IMG_HEIGHT, IMG_WIDTH], method='nearest', preserve_aspect_ratio=True, antialias=True)\n","    resized_img = reshape_as_raster(resized_img)\n","    # print(resized_img.shape, resized_img.dtype, 'amax: {}'.format(np.amax(resized_img)), 'amin: {}'.format(np.amin(resized_img)))\n","\n","    resized_img = rotation(img_fname, resized_img)\n","\n","\n","    ''' update metadata with new size '''\n","    meta.update({\"transform\": transform,\n","                \"height\": resized_img.shape[1],\n","                \"width\": resized_img.shape[2]\n","                })\n","\n","    ''' write the resized image to a new geotiff '''\n","    with rasterio.open(img_save_path/img_fname, 'w', **meta) as dst:\n","      dst.write(resized_img)\n","\n","\n","\n","\n","  ''' Verify contents in source directory are present and assign to iterator '''\n","  src_image_ids = next(os.walk(PATH_SRC_IMAGE))[2]\n","  src_image_ids.sort()\n","  # print('len(src_image_ids): {}'.format(len(src_image_ids)))\n","  # print(src_image_ids[:3])\n","\n","  src_mask_ids = next(os.walk(PATH_SRC_MASK))[2]\n","  src_mask_ids.sort()\n","  # print('len(src_mask_ids): {}'.format(len(src_mask_ids)))\n","  # print(src_mask_ids[:3])\n","\n","\n","  ''' check the target directory for existing files '''\n","  existing_image_ids = next(os.walk(PATH_IMAGE))[2]\n","  existing_image_ids.sort()\n","  # print('len(existing_image_ids): {}'.format(len(existing_image_ids)))\n","  # print(existing_image_ids[:3])\n","\n","  existing_mask_ids = next(os.walk(PATH_MASK))[2]\n","  existing_mask_ids.sort()\n","  # print('len(existing_mask_ids): {}'.format(len(existing_mask_ids)))\n","  # print(existing_mask_ids[:3])\n","\n","\n","  ''' check if resized files already exist, else create them '''\n","  if (existing_image_ids == src_image_ids) and (existing_mask_ids == src_mask_ids): \n","    print('Resized images and masks available\\n')\n","    pass\n","  else:\n","    print('Resized images and masks not available')\n","    print('Resizing image')\n","    for n, id in tqdm(enumerate(src_image_ids), total=len(src_image_ids), position=0, leave=True):\n","      resize_and_save(PATH_SRC_IMAGE, id, PATH_IMAGE)\n","    print('Finished resizing image')\n","\n","    print('Resizing mask')\n","    for n, id in tqdm(enumerate(src_mask_ids), total=len(src_mask_ids), position=0, leave=True):\n","      resize_and_save(PATH_SRC_MASK, id, PATH_MASK)\n","    print('Finished resizing mask')\n","\n","    print('Resized images and masks generated\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lo8S7xw2BKLR","colab_type":"text"},"source":["### gen_train_test_set()\n","This will return the train test set "]},{"cell_type":"code","metadata":{"id":"wr7k5G-p06xa","colab_type":"code","colab":{}},"source":["def gen_train_test_set(PATH_IMAGE, PATH_MASK, IMG_CHANNELS, IMG_DTYPE_SRC, TRAIN_IDX, TEST_IDX, SIZE_TRAIN=SIZE_TRAIN, SIZE_TEST=SIZE_TEST, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH):\n","  print('gen_train_test_set()')  \n","  \n","  X_train_sample_fname = 'sample_X_train_{}.npy'.format(SIZE_TRAIN)\n","  Y_train_sample_fname = 'sample_Y_train_{}.npy'.format(SIZE_TRAIN)\n","  X_test_sample_fname = 'sample_X_test_{}.npy'.format(SIZE_TEST)\n","  Y_test_sample_fname = 'sample_Y_test_{}.npy'.format(SIZE_TEST)\n","  \n","  \n","  ''' load sample data if present '''\n","  try:\n","    X_train = np.load(PATH_SAMPLE/X_train_sample_fname).reshape((SIZE_TRAIN,IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))\n","    Y_train = np.load(PATH_SAMPLE/Y_train_sample_fname).reshape((SIZE_TRAIN,IMG_HEIGHT,IMG_WIDTH,1))\n","    X_test = np.load(PATH_SAMPLE/X_test_sample_fname).reshape((SIZE_TEST,IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))\n","    Y_test = np.load(PATH_SAMPLE/Y_test_sample_fname).reshape((SIZE_TEST,IMG_HEIGHT,IMG_WIDTH,1))\n","    print('Sample data available. Loaded sample data\\n')\n","\n","  except:\n","    ''' get the ids/filenames that we need, based on the random index allocation to avoid grouping '''\n","    ''' X are image files, Y are mask files '''\n","    print('Sample data not available')\n","    all_img_ids = next(os.walk(PATH_IMAGE))[2]\n","    all_img_ids.sort()\n","    all_img_ids = np.array(all_img_ids)\n","\n","    all_mask_ids = next(os.walk(PATH_MASK))[2]\n","    all_mask_ids.sort()\n","    all_mask_ids = np.array(all_mask_ids)\n","\n","    X_train_ids = all_img_ids[TRAIN_IDX]\n","    X_train_ids.sort()\n","    Y_train_ids = all_mask_ids[TRAIN_IDX]\n","    Y_train_ids.sort()\n","\n","    X_test_ids = all_img_ids[TEST_IDX]\n","    X_test_ids.sort()\n","    Y_test_ids = all_mask_ids[TEST_IDX]\n","    Y_test_ids.sort()\n","\n","    # print(len(all_img_ids))\n","    # print(all_img_ids[:3])\n","    # print(len(all_mask_ids))\n","    # print(all_mask_ids[:3])\n","    # print(len(X_train_ids))\n","    # print(X_train_ids[:3])\n","    # print(len(Y_train_ids))\n","    # print(Y_train_ids[:3])\n","    # print(len(X_test_ids))\n","    # print(X_test_ids[:3])\n","    # print(len(Y_test_ids))\n","    # print(Y_test_ids[:3])\n","\n","\n","    ''' prepare numpy arrays to store converted image and mask files. Image files will need to be converted to IMG_DTYPE_SRC and mask files will be converted to np.bool '''\n","    X_train = np.zeros((len(X_train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE_SRC) \n","    Y_train = np.zeros((len(Y_train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n","    X_test = np.zeros((len(X_test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=IMG_DTYPE_SRC) \n","    Y_test = np.zeros((len(Y_test_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n","\n","    # print('X_train', X_train.shape, X_train.dtype)\n","    # print('Y_train', Y_train.shape, Y_train.dtype)\n","    # print('X_test', X_test.shape, X_test.dtype)\n","    # print('Y_test', Y_test.shape, Y_test.dtype)\n","\n","    ''' actually format the imaegs into the correct data shape and type '''\n","    print('formatting training images')\n","    for n, id in tqdm(enumerate(X_train_ids), total=len(X_train_ids), position=0, leave=True):\n","      img = rasterio.open(PATH_IMAGE/id).read()\n","      img = reshape_as_image(img)\n","      X_train[n] = img\n","    print('Finished formatting training images')\n","\n","    print('formatting training masks')\n","    for n, id in tqdm(enumerate(Y_train_ids), total=len(Y_train_ids), position=0, leave=True):\n","      img = rasterio.open(PATH_MASK/id).read()\n","      img = reshape_as_image(img)\n","      img = img.astype(np.bool) \n","      Y_train[n] = img\n","    print('Finished formatting training masks')\n","\n","    print('formatting test images')\n","    for n, id in tqdm(enumerate(X_test_ids), total=len(X_test_ids), position=0, leave=True):\n","      img = rasterio.open(PATH_IMAGE/id).read()\n","      img = reshape_as_image(img)\n","      X_test[n] = img\n","    print('Finished formatting test images')\n","\n","    print('formatting test masks')\n","    for n, id in tqdm(enumerate(Y_test_ids), total=len(Y_test_ids), position=0, leave=True):\n","      img = rasterio.open(PATH_MASK/id).read()\n","      img = reshape_as_image(img)\n","      img = img.astype(np.bool) \n","      Y_test[n] = img\n","    print('Finished formatting test masks')  \n","\n","\n","    np.save(PATH_SAMPLE/X_train_sample_fname, X_train.ravel())\n","    np.save(PATH_SAMPLE/Y_train_sample_fname, Y_train.ravel())\n","    np.save(PATH_SAMPLE/X_test_sample_fname, X_test.ravel())\n","    np.save(PATH_SAMPLE/Y_test_sample_fname, Y_test.ravel())\n","\n","    print('Sample data generated and saved\\n')\n","\n","  return (X_train, Y_train, X_test, Y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PUcz7QchZkVE","colab_type":"text"},"source":["### gen_model()\n","This will generate a basic model to use "]},{"cell_type":"code","metadata":{"id":"LVyjYaKOmG10","colab_type":"code","colab":{}},"source":["def gen_model(MODEL_LOSS, MODEL_BATCH_SIZE, MODEL_EPOCHS, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, IMG_CHANNELS, MODEL_ID=None, MODEL_COMMENTS=''): \n","  MODEL_OPTIMIZER = 'adam'\n","  MODEL_LOSS = JaccardLoss()                                      # CHANGE HERE\n","  MODEL_METRICS =[IOUScore(), FScore()]\n","  MODEL_VAL_SPLIT = 0.1\n","  MODEL_VAL_OFFSET = int(SIZE_TRAIN*(1-MODEL_VAL_SPLIT))    \n","  MODEL_BATCH_SIZE = MODEL_BATCH_SIZE\n","  MODEL_EPOCHS = MODEL_EPOCHS                                               # CHANGE HERE\n","\n","  MODEL_TYPE = 'segmentation-models Unet'\n","  MODEL_PRETRAIN = MODEL_PRETRAIN                                     # CHANGE HERE\n","  MODEL_LOSS_FUNCTION = MODEL_LOSS_FUNCTION                           # CHANGE HERE\n","  MODEL_COMMENTS = MODEL_COMMENTS\n","\n","  if MODEL_ID == None:\n","    MODEL_NAME = 'MODEL_{}__PRETRAIN_{}__LOSS_{}__EPOCH_{}__TRAIN_{}__TEST_{}'.format(MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST)\n","  else:\n","    MODEL_NAME = 'ID_{}__MODEL_{}__PRETRAIN_{}__LOSS_{}__EPOCH_{}__TRAIN_{}__TEST_{}'.format(MODEL_ID, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST)\n","\n","  model = Unet(backbone_name=MODEL_PRETRAIN, encoder_weights=None, input_shape=(None, None, IMG_CHANNELS))\n","  model.compile(optimizer=MODEL_OPTIMIZER, loss=MODEL_LOSS, metrics=MODEL_METRICS)\n","\n","  return (MODEL_OPTIMIZER, MODEL_LOSS, MODEL_METRICS, MODEL_VAL_SPLIT, MODEL_VAL_OFFSET, MODEL_BATCH_SIZE, MODEL_EPOCHS, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_ID, MODEL_NAME, model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQalx2ULxWBH","colab_type":"text"},"source":["### gen_model_results()\n","This will generate a basic model to use "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bgV6w4MZUwLG","colab":{}},"source":["def gen_model_results(PATH_SAVED_MODELS, MODEL_NAME, X_train, Y_train, MODEL_BATCH_SIZE, MODEL_EPOCHS, MODEL_VAL_SPLIT, load_weights_filename='', load_weights_epochs=0, save_model=False):\n","  checkpointer = tf.keras.callbacks.ModelCheckpoint( PATH_SAVED_MODELS/(MODEL_NAME+'_WEIGHTS.h5'), verbose=1, save_best_only=True)\n","  callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', mode='min')]   \n","\n","  if load_weights_filename == '':\n","    print('No weights loaded. Training new model')\n","    results = model.fit(X_train, Y_train, batch_size=MODEL_BATCH_SIZE, epochs=MODEL_EPOCHS, validation_split=MODEL_VAL_SPLIT, callbacks=callbacks)\n","  else:\n","    model.load_weights(str(PATH_SAVED_MODELS/load_weights_filename))\n","\n","    # load_weights_epochs: the extra epochs to train for, after loading in weights. \n","    if load_weights_epochs == 0:\n","      print('Just loading weights')\n","      results = None\n","    else:\n","      print('Weights loaded, training for extra {} epochs'.format(load_weights_epochs))\n","      results = model.fit(X_train, Y_train, batch_size=MODEL_BATCH_SIZE, epochs=load_weights_epochs, validation_split=MODEL_VAL_SPLIT, callbacks=callbacks)\n","\n","  if save_model == True:\n","    # model.save(str(PATH_SAVED_MODELS/(MODEL_NAME+'.h5')))\n","    model.save_weights(str(PATH_SAVED_MODELS/(MODEL_NAME+'_WEIGHTS.h5')))\n","    print('model saved')\n","  \n","  return (results, model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkpqSyezZoN3","colab_type":"text"},"source":["### gen_predictions()\n","This will generate the predictions based on a trained model"]},{"cell_type":"code","metadata":{"id":"5Na6f_Frx-KJ","colab_type":"code","colab":{}},"source":["def gen_predictions(model, X_train, Y_train, X_test, Y_test, MODEL_VAL_OFFSET, IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_BATCH_SIZE, MODEL_EPOCHS, SIZE_TRAIN=SIZE_TRAIN, SIZE_TEST=SIZE_TEST, PATH_METRICS=PATH_METRICS):\n","  ''' split train, val 90-10. validation_split takes the percentage from the back '''\n","  preds_train = model.predict(X_train[:MODEL_VAL_OFFSET], verbose=1)\n","  preds_val = model.predict(X_train[MODEL_VAL_OFFSET:], verbose=1)\n","  preds_test = model.predict(X_test, verbose=1)\n","\n","  preds_train_t = (preds_train > 0.5).astype(np.uint8)\n","  preds_val_t = (preds_val > 0.5).astype(np.uint8)\n","  preds_test_t = (preds_test > 0.5).astype(np.uint8)\n","\n","\n","  ''' evaluation of prediction '''\n","  metric_names = model.metrics_names\n","  \n","  print(\"train set evaluation\")\n","  train_metrics = model.evaluate(X_train[:MODEL_VAL_OFFSET], Y_train[:MODEL_VAL_OFFSET])\n","  for x in range(len(metric_names)):\n","    print(metric_names[x], ': ', train_metrics[x])\n","\n","  print(\"\\nvalidation set evaluation\")\n","  val_metrics = model.evaluate(X_train[MODEL_VAL_OFFSET:], X_train[MODEL_VAL_OFFSET:])\n","  for x in range(len(metric_names)):\n","    print(metric_names[x], ': ', val_metrics[x])\n","\n","  print(\"\\ntest set evaluation\")\n","  test_metrics = model.evaluate(X_test, Y_test)\n","  for x in range(len(metric_names)):\n","    print(metric_names[x], ': ', test_metrics[x])\n","\n","  store_metrics(test_metrics[0], test_metrics[1], test_metrics[2], val_metrics[0], val_metrics[1], val_metrics[2], train_metrics[0], train_metrics[1], train_metrics[2], IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_BATCH_SIZE, MODEL_EPOCHS, SIZE_TRAIN, SIZE_TEST, PATH_METRICS)\n","\n","  return (preds_train_t, preds_val_t, preds_test_t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KCl-t5tPtmYJ"},"source":["## Bring in the SAR data, both rotated and un-rotated\n","The geotiff metadata has probably been copied wrongly for now for rotated images"]},{"cell_type":"code","metadata":{"id":"TWM3khYqEckA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1596036695562,"user_tz":-480,"elapsed":30001,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"2aa877c4-71d3-4110-ed33-9cbdb48b8cb9"},"source":["''' run when changing parameters '''\n","IMG_TYPE, IMG_WIDTH, IMG_HEIGHT, IMG_ROTATED, IMG_CHANNELS, IMG_DTYPE_SRC, IMG_DTYPE = gen_input_image_details('SAR-Intensity', False)       # CHANGE HERE\n","\n","PATH_SRC, PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_DATA, PATH_IMAGE, PATH_MASK, PATH_SAMPLE, PATH_SAVED_MODELS = gen_paths(IMG_TYPE, IMG_ROTATED)\n","gen_resized_images(PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_IMAGE, PATH_MASK)\n","X_train_no_rotate, Y_train_no_rotate, X_test_no_rotate, Y_test_no_rotate = gen_train_test_set(PATH_IMAGE, PATH_MASK, IMG_CHANNELS, IMG_DTYPE_SRC, TRAIN_IDX, TEST_IDX)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gen_input_image_details()\n","IMG_TYPE: SAR-Intensity, IMG_HEIGHT: 256, IMG_WIDTH: 256, IMG_ROTATED: False, IMG_CHANNELS: 4, IMG_DTYPE_SRC: <class 'numpy.float32'>, IMG_DTYPE: <class 'numpy.uint8'>\n","\n","gen_paths()\n","PATH_SRC: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam, \n","PATH_SRC_IMAGE: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam/SAR-Intensity, \n","PATH_SRC_MASK: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam/_mask, \n","PATH_DATA: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data/SAR-Intensity_256x256, \n","PATH_IMAGE: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data/SAR-Intensity_256x256/image, \n","PATH_MASK: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data/SAR-Intensity_256x256/mask, \n","PATH_SAMPLE: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data/SAR-Intensity_256x256/sample, \n","PATH_SAVED_MODELS: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data/SAR-Intensity_256x256/saved_models\n","\n","gen_resized_images()\n","Resized images and masks available\n","\n","gen_train_test_set()\n","Sample data available. Loaded sample data\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yn2UQlkKneRS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1596036721833,"user_tz":-480,"elapsed":56080,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"1a3f6fd1-49b9-4019-b46f-baf12747cf10"},"source":["''' run when changing parameters '''\n","IMG_TYPE, IMG_WIDTH, IMG_HEIGHT, IMG_ROTATED, IMG_CHANNELS, IMG_DTYPE_SRC, IMG_DTYPE = gen_input_image_details('SAR-Intensity', True)       # CHANGE HERE\n","\n","PATH_SRC, PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_DATA, PATH_IMAGE, PATH_MASK, PATH_SAMPLE, PATH_SAVED_MODELS = gen_paths(IMG_TYPE, IMG_ROTATED)\n","gen_resized_images(PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_IMAGE, PATH_MASK)\n","X_train_rotate, Y_train_rotate, X_test_rotate, Y_test_rotate = gen_train_test_set(PATH_IMAGE, PATH_MASK, IMG_CHANNELS, IMG_DTYPE_SRC, TRAIN_IDX, TEST_IDX)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gen_input_image_details()\n","IMG_TYPE: SAR-Intensity, IMG_HEIGHT: 256, IMG_WIDTH: 256, IMG_ROTATED: True, IMG_CHANNELS: 4, IMG_DTYPE_SRC: <class 'numpy.float32'>, IMG_DTYPE: <class 'numpy.uint8'>\n","\n","gen_paths()\n","PATH_SRC: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam, \n","PATH_SRC_IMAGE: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam/SAR-Intensity, \n","PATH_SRC_MASK: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_BACKUP/train_all/AOI_11_Rotterdam/_mask, \n","PATH_DATA: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_rotated/SAR-Intensity_256x256, \n","PATH_IMAGE: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_rotated/SAR-Intensity_256x256/image, \n","PATH_MASK: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_rotated/SAR-Intensity_256x256/mask, \n","PATH_SAMPLE: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_rotated/SAR-Intensity_256x256/sample, \n","PATH_SAVED_MODELS: /content/gdrive/My Drive/Colab Notebooks/spacenet6/spacenet6_data_rotated/SAR-Intensity_256x256/saved_models\n","\n","gen_resized_images()\n","Resized images and masks available\n","\n","gen_train_test_set()\n","Sample data available. Loaded sample data\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dMgUc_tmKmuH","colab_type":"text"},"source":["Boolean masks to float32 masks\n","Package updates as of 4 Aug 2020 rejected the boolean masks, so convert them into np.float32 to match with the X data"]},{"cell_type":"code","metadata":{"id":"XqH9e7SLKqor","colab_type":"code","colab":{}},"source":["Y_train_rotate = Y_train_rotate.astype(np.float32)\n","Y_test_rotate = Y_test_rotate.astype(np.float32)\n","Y_train_no_rotate = Y_train_no_rotate.astype(np.float32)\n","Y_test_no_rotate = Y_test_no_rotate.astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85ueVatR0TC8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596036721834,"user_tz":-480,"elapsed":55880,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"2402ea66-f0ef-4059-aa5a-d8c7a055bbf1"},"source":["print(X_train_no_rotate.shape, Y_train_no_rotate.shape, X_test_no_rotate.shape, Y_test_no_rotate.shape)\n","print(X_train_rotate.shape, Y_train_rotate.shape, X_test_rotate.shape, Y_test_rotate.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(800, 256, 256, 4) (800, 256, 256, 1) (80, 256, 256, 4) (80, 256, 256, 1)\n","(800, 256, 256, 4) (800, 256, 256, 1) (80, 256, 256, 4) (80, 256, 256, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ow28nkeYnvn8","colab_type":"text"},"source":["# EXPERIMENT"]},{"cell_type":"markdown","metadata":{"id":"YQ7sG8u4dy9z","colab_type":"text"},"source":["**There are lots of things that we can experiment with right now**\n","* float32 vs uint8\n","* float32 vs float32 normalized to 0,1\n","* rotation vs no rotation\n","* early stopping: mode=max, val_iou_score\n","* various backbones\n","  *  resnet18 / 34 / 50 / 101 / 152\n","  *  seresnet18 / 34 / 50 / 101 / 152\n","  *  seresnext50 / 101 / 154 / 50 / 101\n","  *  vgg16 / 19\n","  *  densenet121 / 169 / 201\n","  *  inceptionresnetv2 / 3\n","  *  mobilenet / v2\n","  *  efficientnetb0 / 1 / 2 / 3 / 4 / 5 / 6 / 7\n","*various loss functions \n","  *  jaccardloss\n","  *  diceloss (play with beta)\n","  *  binaryceloss\n","  *  binaryfocalloss (play with alpha, gamma)\n","    "]},{"cell_type":"markdown","metadata":{"id":"ZQRJOZjpesXp","colab_type":"text"},"source":["**focal loss**\n","\n","α and γ are hyperparameters that can be tweaked for further calibration. γ can also be said as a relaxation parameter in laymen’s terms.\n","More the value of γ, more importance will be given to misclassified examples and very less loss will be propagated from easy examples. According to the study mentioned in [5], γ=2 gives the best results.\n","\n","α is used to specify the weight of different categories/labels, the size of the array needs to be consistent with the number of classes.\n","\n","**dice loss**\n","\n","More specifically, in the case of β = 1 the Fβ index simplifies to be the Dice loss function (F1) while β = 2 generates the F2 score and β = 0 transforms the function to precision. Larger β weighs recall higher than precision (by placing more emphasis on false negatives).\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H3Z8D46grPFz"},"source":["## Experiment details\n","patience=10, epochs=100, batchsize=4, size=800-80, SAR not rotated\n","\n","Experiment 1L, 1M, 1N: resnet50, efficientnetb3\n","\n","All will try JaccardLoss(), DiceLoss(beta=1.25), BinaryCELoss(), BinaryFocalLoss(alpha=0.75, gamma= 0.75)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"80qvuWKwIK6E"},"source":["## Experiment XXX: resnet50"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sz9jWUE5IK6G"},"source":["### ID XXX. JaccardLoss()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EEH61fz0IK6G","colab":{}},"source":["''' set the train/test ''' \n","IMG_TYPE, IMG_WIDTH, IMG_HEIGHT, IMG_ROTATED, IMG_CHANNELS, IMG_DTYPE_SRC, IMG_DTYPE = gen_input_image_details('SAR-Intensity', False)       # CHANGE HERE\n","PATH_SRC, PATH_SRC_IMAGE, PATH_SRC_MASK, PATH_DATA, PATH_IMAGE, PATH_MASK, PATH_SAMPLE, PATH_SAVED_MODELS = gen_paths(IMG_TYPE, IMG_ROTATED)\n","if IMG_ROTATED:\n","  X_train, Y_train, X_test, Y_test = X_train_rotate, Y_train_rotate, X_test_rotate, Y_test_rotate\n","else:\n","  X_train, Y_train, X_test, Y_test = X_train_no_rotate, Y_train_no_rotate, X_test_no_rotate, Y_test_no_rotate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"afrHRb83IK6J","colab":{}},"source":["''' set the model parameters, train model '''\n","MODEL_OPTIMIZER, MODEL_LOSS, MODEL_METRICS, MODEL_VAL_SPLIT, MODEL_VAL_OFFSET, MODEL_BATCH_SIZE, MODEL_EPOCHS, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_ID, MODEL_NAME, model = gen_model(\n","    JaccardLoss(), 4, 100, 'segmentation-models Unet', 'resnet50', 'JaccardLoss()', IMG_CHANNELS, MODEL_ID=XXX, MODEL_COMMENTS='resnet50 test')          # CHANGE HERE\n","\n","results, model = gen_model_results(PATH_SAVED_MODELS, MODEL_NAME, X_train, Y_train, MODEL_BATCH_SIZE, MODEL_EPOCHS, MODEL_VAL_SPLIT, \n","                                   load_weights_filename='', load_weights_epochs=0, save_model=True)                                            # CHANGE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_IqANexqIK6L","colab":{}},"source":["''' predict using model '''\n","preds_train_t, preds_val_t, preds_test_t = gen_predictions(model, X_train, Y_train, X_test, Y_test, MODEL_VAL_OFFSET, IMG_TYPE, IMG_ROTATED, MODEL_NAME, MODEL_TYPE, MODEL_PRETRAIN, MODEL_LOSS_FUNCTION, MODEL_COMMENTS, MODEL_BATCH_SIZE, MODEL_EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3bZnVl-JIK6N","colab":{}},"source":["''' plot predictions to visualize ''' \n","plot_XY_preds('train preds', X_train[:MODEL_VAL_OFFSET], Y_train[:MODEL_VAL_OFFSET], preds_train_t, 'X_train', 'Y_train', 'train prediction')\n","plot_XY_preds('val preds', X_train[MODEL_VAL_OFFSET:], Y_train[MODEL_VAL_OFFSET:], preds_val_t, 'X_val', 'Y_val', 'val prediction')\n","plot_XY_preds('test preds', X_test, Y_test, preds_test_t, 'X_test', 'Y_test', 'test prediction')\n","if results != None:\n","  plot_history(results.history)"],"execution_count":null,"outputs":[]}]}